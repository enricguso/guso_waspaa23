{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import masp as srs\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import scipy.signal as sig\n",
    "import copy\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import mat73\n",
    "import pyrubberband as pyrb\n",
    "import helpers as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp);\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = df.iloc[i]\n",
    "def process(a):\n",
    "    mic = np.array(hlp.head_2_ku_ears(np.array([a.headC_x, a.headC_y, a.headC_z]),\n",
    "                                            np.array([a.headOrient_azi,a.headOrient_ele])))\n",
    "    # load noise:\n",
    "    noise, _ = sf.read(pjoin(pjoin(pjoin(wham_path, 'wham_noise'), a.wham_split), a.noise_path))\n",
    "\n",
    "    # time stretch if needed\n",
    "    if a.stretch != 0.0:\n",
    "        noise = pyrb.time_stretch(noise, a.fs_noise, a.stretch)\n",
    "\n",
    "    # extend if needed with hanning window\n",
    "    noise = np.array([hlp.extend_noise(noise[:,0], a.num_chunks * 4 * a.fs_noise, a.fs_noise),\n",
    "            hlp.extend_noise(noise[:,1], a.num_chunks * 4 * a.fs_noise, a.fs_noise)]).T\n",
    "    # crop 4 seconds chunk\n",
    "    noise = noise[a.chunk * 4 * a.fs_noise :(a.chunk + 1) * 4 * a.fs_noise]\n",
    "\n",
    "    # invert phase for augmentation\n",
    "    if a.phase_inv:\n",
    "        noise *= -1\n",
    "\n",
    "    # invert channels for augmentation\n",
    "    if a.lr_inv:\n",
    "        noise = noise[:, [1,0]]\n",
    "\n",
    "    noise = noise.T\n",
    "\n",
    "    # load speech and crop at the 4s chunk that has more energy\n",
    "    speech_folder = pjoin(pjoin(mls_path, a.mls_split), 'audio')\n",
    "    speech, _ = sf.read(pjoin(pjoin(pjoin(speech_folder, str(a.speaker)), str(a.book)), a.speech_path))\n",
    "    env = sig.fftconvolve(speech, np.ones(4*a.fs_noise), 'same')\n",
    "    idx_candidates = np.flip(np.argsort(env**2))\n",
    "    idx = idx_candidates[idx_candidates < (len(speech)-(4*a.fs_noise))][0]\n",
    "    speech = speech[idx:idx+4*a.fs_noise]\n",
    "\n",
    "    room = np.array([a.room_x, a.room_y, a.room_z])\n",
    "    rt60 = np.array([a.rt60])\n",
    "    rt60 *= 0.5 #furniture absorption \n",
    "    #snr 0, more people, more reduction -> 0.3 * rt60\n",
    "    #snr 5, less people, no rt60 reduction -> 1.0 * rt60\n",
    "    rt60 *= ((a.snr+0.3)/5.3) # people absoprtion\n",
    "    src = np.array([[a.src_x, a.src_y, a.src_z]])\n",
    "    head_orient = np.array([a.headOrient_azi, a.headOrient_ele])\n",
    "\n",
    "    # Compute absorption coefficients for desired rt60 and room dimensions\n",
    "    abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60)\n",
    "    # Small correction for sound absorption coefficients:\n",
    "    if sum(rt60_true-rt60>0.05*rt60_true)>0 :\n",
    "        abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60_true + abs(rt60-rt60_true))\n",
    "\n",
    "    # Generally, we simulate up to RT60:\n",
    "    limits = np.minimum(rt60, maxlim)\n",
    "    # Compute IRs with MASP at 48k:\n",
    "    abs_echograms = srs.compute_echograms_sh(room, src, mic, abs_walls, limits, ambi_order, rims_d, head_orient)\n",
    "    ane_echograms = hlp.crop_echogram(copy.deepcopy(abs_echograms))\n",
    "    mic_rirs = srs.render_rirs_sh(abs_echograms, band_centerfreqs, fs_rir)/np.sqrt(4*np.pi)\n",
    "    ane_rirs = srs.render_rirs_sh(ane_echograms, band_centerfreqs, fs_rir)/np.sqrt(4*np.pi)\n",
    "\n",
    "    # Decode SH IRs to binaural\n",
    "    bin_ir = np.array([sig.fftconvolve(np.squeeze(mic_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                        sig.fftconvolve(np.squeeze(mic_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "    bin_aneIR = np.array([sig.fftconvolve(np.squeeze(ane_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                        sig.fftconvolve(np.squeeze(ane_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "\n",
    "    # Apply to the source signal\n",
    "    reverberant_src = np.array([sig.fftconvolve(sig.resample_poly(speech,fs_rir,fs_target), bin_ir[0, :], 'same'), sig.fftconvolve(sig.resample_poly(speech,fs_rir,fs_target), bin_ir[1, :], 'same')])\n",
    "    anechoic_src = np.array([sig.fftconvolve(sig.resample_poly(speech,fs_rir,fs_target), bin_aneIR[0, :], 'same'), sig.fftconvolve(sig.resample_poly(speech,fs_rir,fs_target), bin_aneIR[1, :], 'same')])\n",
    "\n",
    "    # Downsample to 16k:\n",
    "    reverberant_src = np.array([sig.resample_poly(reverberant_src[0], fs_target, fs_rir), \n",
    "                        sig.resample_poly(reverberant_src[1], fs_target, fs_rir)])\n",
    "    anechoic_src = np.array([sig.resample_poly(anechoic_src[0], fs_target, fs_rir), \n",
    "                        sig.resample_poly(anechoic_src[1], fs_target, fs_rir)])\n",
    "    # Apply RIC correction bell filter at 2kHz resonance:\n",
    "    reverberant_src = np.array([sig.lfilter(filt_b, filt_a, reverberant_src[0]), sig.lfilter(filt_b, filt_a, reverberant_src[1])])\n",
    "\n",
    "    anechoic_src = np.array([sig.lfilter(filt_b, filt_a, anechoic_src[0]), sig.lfilter(filt_b, filt_a, anechoic_src[1])])\n",
    "\n",
    "    # Apply SNR:\n",
    "    ini_snr = 10 * np.log10(hlp.power(reverberant_src) / hlp.power(noise) + np.finfo(noise.dtype).resolution)\n",
    "    noise_gain_db = ini_snr - a.snr\n",
    "\n",
    "    noise = noise * np.power(10, noise_gain_db/20)\n",
    "\n",
    "    # Amplitude normalization:\n",
    "    norm_fact = np.max(np.abs(reverberant_src + noise))\n",
    "    anechoic_src /= norm_fact\n",
    "    noise /= norm_fact\n",
    "    reverberant_src /= norm_fact\n",
    "\n",
    "    anechoic_src *= 0.99\n",
    "    noise *= 0.99\n",
    "    reverberant_src *= 0.99\n",
    "\n",
    "    writepath = pjoin(output_path, a.mls_split)\n",
    "    sf.write(pjoin(pjoin(writepath, 'anechoic'), os.path.splitext(a.speech_path)[0]+'.wav'), anechoic_src.T, fs_target, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'reverberant'), os.path.splitext(a.speech_path)[0]+'.wav'), reverberant_src.T, fs_target, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'noise'), os.path.splitext(a.speech_path)[0]+'.wav'), noise.T, fs_target, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'ir'), os.path.splitext(a.speech_path)[0]+'.wav'), bin_ir.T, fs_target, subtype='FLOAT')\n",
    "    sf.write(pjoin(pjoin(writepath, 'ane_ir'), os.path.splitext(a.speech_path)[0]+'.wav'), bin_aneIR.T, fs_target, subtype='FLOAT')\n",
    "    # Add mono IRs for other works:\n",
    "    sf.write(pjoin(pjoin(writepath, 'mono_ir'), os.path.splitext(a.speech_path)[0]+'.wav'), mic_rirs[:, 0, 0, 0], fs_target, subtype='FLOAT')\n",
    "    print('Processed ' + str(a.idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = 'meta_microson_v1.csv'\n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_path = 'decoders_ord10/RIC_Front_Omni_ALFE_Window_SinEQ_bimag.mat'\n",
    "mls_path = '/home/ubuntu/Data/mls_spanish'\n",
    "wham_path = '/home/ubuntu/Data/wham'\n",
    "output_path = '/home/ubuntu/Data/microson_v1/'\n",
    "fs_rir = 48000\n",
    "fs_target = 16000 \n",
    "ambi_order = 10\n",
    "rims_d = .0\n",
    "maxlim = 2.\n",
    "band_centerfreqs=np.array([1000])\n",
    "decoder = mat73.loadmat(decoder_path)['hnm']\n",
    "filt_b, filt_a = hlp.bell(2300, fs_target, np.power(10, -18/20), 8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we list all files we could not process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = os.listdir(pjoin(pjoin(output_path, 'train'), 'reverberant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31536eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = [os.path.splitext(x)[0]+'.flac' for x in processed_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_files = df[~df['speech_path'].isin(processed_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08178f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_files[unprocessed_files['mls_split']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c9402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_files = unprocessed_files[unprocessed_files['mls_split']=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f20383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(len(unprocessed_files))):\n",
    "    process(unprocessed_files.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf7197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KU100 DECODERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73718e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.iloc[3]\n",
    "hlp.plot_scene(np.array([a.room_x, a.room_y, a.room_z]), \n",
    "              np.array([a.headC_x, a.headC_y, a.headC_z]),\n",
    "              np.array([a.headOrient_azi, a.headOrient_ele]),\n",
    "              hlp.head_2_ku_ears(np.array([a.headC_x, a.headC_y, a.headC_z]),\n",
    "              np.array([a.headOrient_azi, a.headOrient_ele])),\n",
    "              np.array([[a.src_x, a.src_y, a.src_z]])) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('Ku100_Alfe_Window_Dense.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, False), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39975d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('Ku100_Alfe_Window.mat')['hnm']\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, False), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('Ku100_noALFE_noWindow.mat')['hnm']\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, False), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('Ku100_ALFE_Window_sinEQ.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, False), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_Alfe_Window_Dense.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_Alfe_Window.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbdc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_noALFE_noWindow.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_Front_Omni_ALFE_Window_SinEQ.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bfc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_Front_Omni_ALFE_Window_SinEQ.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibriMix3D",
   "language": "python",
   "name": "librimix3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
