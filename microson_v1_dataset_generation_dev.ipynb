{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import masp as srs\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import scipy.signal as sig\n",
    "import copy\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import mat73\n",
    "import tqdm\n",
    "import pyrubberband as pyrb\n",
    "import helpers as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bell(fc, fs, gain, Q):\n",
    "    wc = 2 * np.pi * fc / fs\n",
    "    c = 1.0 / np.tan(wc / 2.0)\n",
    "    phi = c*c\n",
    "    Knum = c / Q\n",
    "    Kdenom = Knum\n",
    "\n",
    "    if (gain > 1.0):\n",
    "        Knum *= gain\n",
    "    elif (gain < 1.0):\n",
    "        Kdenom /= gain\n",
    "\n",
    "    a0 = phi + Kdenom + 1.0\n",
    "\n",
    "    b = [(phi + Knum + 1.0) / a0, 2.0 *\n",
    "         (1.0 - phi) / a0, (phi - Knum + 1.0) / a0]\n",
    "    a = [1, 2.0 * (1.0 - phi) / a0, (phi - Kdenom + 1.0) / a0]\n",
    "\n",
    "    return np.asarray(b), np.asarray(a)\n",
    "\n",
    "#a = df.iloc[i]\n",
    "def process(a, correct):\n",
    "\n",
    "    mic = np.array(hlp.head_2_ku_ears(np.array([a.headC_x, a.headC_y, a.headC_z]),\n",
    "                                        np.array([a.headOrient_azi,a.headOrient_ele])))\n",
    "    # load noise:\n",
    "    noise, _ = sf.read(pjoin(pjoin(pjoin(wham_path, 'wham_noise'), a.wham_split), a.noise_path))\n",
    "\n",
    "    # time stretch if needed\n",
    "    if a.stretch != 0.0:\n",
    "        noise = pyrb.time_stretch(noise, a.fs_noise, a.stretch)\n",
    "\n",
    "    # extend if needed with hanning window\n",
    "    noise = np.array([hlp.extend_noise(noise[:,0], a.num_chunks * 4 * a.fs_noise, a.fs_noise),\n",
    "            hlp.extend_noise(noise[:,1], a.num_chunks * 4 * a.fs_noise, a.fs_noise)]).T\n",
    "    # crop 4 seconds chunk\n",
    "    noise = noise[a.chunk * 4 * a.fs_noise :(a.chunk + 1) * 4 * a.fs_noise]\n",
    "\n",
    "    # invert phase for augmentation\n",
    "    if a.phase_inv:\n",
    "        noise *= -1\n",
    "\n",
    "    # invert channels for augmentation\n",
    "    if a.lr_inv:\n",
    "        noise = noise[:, [1,0]]\n",
    "\n",
    "    noise = noise.T\n",
    "\n",
    "    # load speech and crop at the 4s chunk that has more energy\n",
    "    speech_folder = pjoin(pjoin(mls_path, a.mls_split), 'audio')\n",
    "    speech, _ = sf.read(pjoin(pjoin(pjoin(speech_folder, str(a.speaker)), str(a.book)), a.speech_path))\n",
    "    env = sig.fftconvolve(speech, np.ones(4*a.fs_noise), 'same')\n",
    "    idx_candidates = np.flip(np.argsort(env**2))\n",
    "    idx = idx_candidates[idx_candidates < (len(speech)-(4*a.fs_noise))][0]\n",
    "    speech = speech[idx:idx+4*a.fs_noise]\n",
    "\n",
    "    room = np.array([a.room_x, a.room_y, a.room_z])\n",
    "    rt60 = np.array([a.rt60])\n",
    "    rt60 *= 0.5#furniture absorption? \n",
    "    #snr 0, more people, more reduction -> 0.3 * rt60\n",
    "    #snr 5, less people, no rt60 reduction -> 1.0 * rt60\n",
    "    rt60 *= ((a.snr+0.3)/5.3) # people absoprtion\n",
    "    src = np.array([[a.src_x, a.src_y, a.src_z]])\n",
    "    head_orient = np.array([a.headOrient_azi, a.headOrient_ele])\n",
    "\n",
    "    # Compute absorption coefficients for desired rt60 and room dimensions\n",
    "    abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60)\n",
    "    # Small correction for sound absorption coefficients:\n",
    "    if sum(rt60_true-rt60>0.05*rt60_true)>0 :\n",
    "        abs_walls,rt60_true = srs.find_abs_coeffs_from_rt(room, rt60_true + abs(rt60-rt60_true))\n",
    "\n",
    "    # Generally, we simulate up to RT60:\n",
    "    limits = np.minimum(rt60, maxlim)\n",
    "    abs_echograms = srs.compute_echograms_sh(room, src, mic, abs_walls, limits, ambi_order, rims_d, head_orient)\n",
    "    ane_echograms = hlp.crop_echogram(copy.deepcopy(abs_echograms))\n",
    "    mic_rirs = srs.render_rirs_sh(abs_echograms, band_centerfreqs, fs_rir)/np.sqrt(4*np.pi)\n",
    "    ane_rirs = srs.render_rirs_sh(ane_echograms, band_centerfreqs, fs_rir)/np.sqrt(4*np.pi)\n",
    "\n",
    "    # Decode SH IRs to binaural\n",
    "    bin_ir = np.array([sig.fftconvolve(np.squeeze(mic_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                        sig.fftconvolve(np.squeeze(mic_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "    bin_aneIR = np.array([sig.fftconvolve(np.squeeze(ane_rirs[:,:,0, 0]), decoder[:,:,0], 'full', 0).sum(1),\n",
    "                        sig.fftconvolve(np.squeeze(ane_rirs[:,:,1, 0]), decoder[:,:,1], 'full', 0).sum(1)])\n",
    "    \n",
    "    # Apply to the source signal\n",
    "    reverberant_src = np.array([sig.fftconvolve(sig.resample_poly(speech,fs_rir,fs_target), bin_ir[0, :], 'same'), sig.fftconvolve(sig.resample_poly(speech,fs_rir,fs_target), bin_ir[1, :], 'same')])\n",
    "    anechoic_src = np.array([sig.fftconvolve(sig.resample_poly(speech,fs_rir,fs_target), bin_aneIR[0, :], 'same'), sig.fftconvolve(sig.resample_poly(speech,fs_rir,fs_target), bin_aneIR[1, :], 'same')])\n",
    "    \n",
    "    # Downsample again:\n",
    "    reverberant_src = np.array([sig.resample_poly(reverberant_src[0], fs_target, fs_rir), \n",
    "                         sig.resample_poly(reverberant_src[1], fs_target, fs_rir)])\n",
    "    anechoic_src = np.array([sig.resample_poly(anechoic_src[0], fs_target, fs_rir), \n",
    "                         sig.resample_poly(anechoic_src[1], fs_target, fs_rir)])\n",
    "    if correct:\n",
    "    # Apply RIC correction\n",
    "        reverberant_src = np.array([sig.lfilter(filt_b, filt_a, reverberant_src[0]), sig.lfilter(filt_b, filt_a, reverberant_src[1])])\n",
    "\n",
    "        anechoic_src = np.array([sig.lfilter(filt_b, filt_a, anechoic_src[0]), sig.lfilter(filt_b, filt_a, anechoic_src[1])])\n",
    "\n",
    "    ini_snr = 10 * np.log10(hlp.power(reverberant_src) / hlp.power(noise) + np.finfo(noise.dtype).resolution)\n",
    "    noise_gain_db = ini_snr - a.snr\n",
    "\n",
    "    noise = noise * np.power(10, noise_gain_db/20)\n",
    "    norm_fact = np.max(np.abs(reverberant_src + noise))\n",
    "\n",
    "    anechoic_src /= norm_fact\n",
    "    noise /= norm_fact\n",
    "    reverberant_src /= norm_fact\n",
    "\n",
    "    anechoic_src *= 0.99\n",
    "    noise *= 0.99\n",
    "    reverberant_src *= 0.99\n",
    "    return reverberant_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = 'meta_microson_v1.csv'\n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_path = 'ku100_ha_test.mat'\n",
    "mls_path = '/home/ubuntu/Data/mls_spanish'\n",
    "wham_path = '/home/ubuntu/Data/wham'\n",
    "output_path = '/home/ubuntu/Data/microson_v1/'\n",
    "fs_rir = 48000\n",
    "fs_target = 16000 \n",
    "ambi_order = 10\n",
    "rims_d = .0\n",
    "maxlim = 2.\n",
    "band_centerfreqs=np.array([1000])\n",
    "decoder = mat73.loadmat(decoder_path)['hnm']\n",
    "filt_b, filt_a = bell(2300, fs_target, np.power(10, -18/20), 8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KU100 DECODERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73718e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.iloc[3]\n",
    "hlp.plot_scene(np.array([a.room_x, a.room_y, a.room_z]), \n",
    "              np.array([a.headC_x, a.headC_y, a.headC_z]),\n",
    "              np.array([a.headOrient_azi, a.headOrient_ele]),\n",
    "              hlp.head_2_ku_ears(np.array([a.headC_x, a.headC_y, a.headC_z]),\n",
    "              np.array([a.headOrient_azi, a.headOrient_ele])),\n",
    "              np.array([[a.src_x, a.src_y, a.src_z]])) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('Ku100_Alfe_Window_Dense.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, False), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39975d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('Ku100_Alfe_Window.mat')['hnm']\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, False), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('Ku100_noALFE_noWindow.mat')['hnm']\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, False), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('Ku100_ALFE_Window_sinEQ.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, False), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_Alfe_Window_Dense.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_Alfe_Window.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbdc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_noALFE_noWindow.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_Front_Omni_ALFE_Window_SinEQ.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bfc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = mat73.loadmat('RIC_Front_Omni_ALFE_Window_SinEQ.mat')['hnm'] # weird\n",
    "decoder = np.roll(decoder,500,axis=0)\n",
    "#plt.plot(decoder[:,:,0])\n",
    "Audio(process(a, True), rate=fs_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibriMix3D",
   "language": "python",
   "name": "librimix3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
