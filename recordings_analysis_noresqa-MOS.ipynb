{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.fft\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import resample_poly\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "import tqdm\n",
    "# import my modules (helpers.py where I stored all the functions):\n",
    "import helpers as hlp\n",
    "import importlib \n",
    "importlib.reload(hlp)\n",
    "from clarity.evaluator.hasqi import hasqi_v2\n",
    "from clarity.evaluator.haspi import haspi_v2 \n",
    "from clarity.evaluator.mbstoi import mbstoi \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from noresqa import NORESQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_type = 1 #NORESQA-MOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = 'noresqa_models/model_noresqa_mos.pth'\n",
    "state = torch.load(model_checkpoint_path,map_location=\"cpu\")['state_dict']\n",
    "\n",
    "model = NORESQA(output=40, output2=40, metric_type = metric_type, config_path = 'noresqa_models/wav2vec_small.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = {}\n",
    "for k, v in state.items():\n",
    "    if 'module' in k:\n",
    "        pretrained_dict[k.replace('module.','')]=v\n",
    "    else:\n",
    "        pretrained_dict[k]=v\n",
    "model_dict = model.state_dict()\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(pretrained_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change device as needed\n",
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfmax = nn.Softmax(dim=1)\n",
    "# function extraction stft\n",
    "def extract_stft(audio, sampling_rate = 16000):\n",
    "\n",
    "    fx, tx, stft_out = signal.stft(audio, sampling_rate, window='hann',nperseg=512,noverlap=256,nfft=512)\n",
    "    stft_out = stft_out[:256,:]\n",
    "    feat = np.concatenate((np.abs(stft_out).reshape([stft_out.shape[0],stft_out.shape[1],1]), np.angle(stft_out).reshape([stft_out.shape[0],stft_out.shape[1],1])), axis=2)\n",
    "    return feat\n",
    "\n",
    "# noresqa and noresqa-mos prediction calls\n",
    "def model_prediction_noresqa(test_feat, nmr_feat):\n",
    "\n",
    "    intervals_sdr = np.arange(0.5,40,1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ranking_frame,sdr_frame,snr_frame = model(test_feat.permute(0,3,2,1),nmr_feat.permute(0,3,2,1))\n",
    "        # preference task prediction\n",
    "        ranking = sfmax(ranking_frame).mean(2).detach().cpu().numpy()\n",
    "        pout = ranking[0][0]\n",
    "        # quantification task\n",
    "        sdr = intervals_sdr * (sfmax(sdr_frame).mean(2).detach().cpu().numpy())\n",
    "        qout = sdr.sum()\n",
    "\n",
    "    return pout, qout\n",
    "\n",
    "def model_prediction_noresqa_mos(test_feat, nmr_feat):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        score = model(nmr_feat,test_feat).detach().cpu().numpy()[0]\n",
    "\n",
    "    return score\n",
    "\n",
    "# reading audio clips\n",
    "def audio_loading(path,sampling_rate=16000):\n",
    "\n",
    "    audio, fs = librosa.load(path, sr=None)\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = librosa.to_mono(audio)\n",
    "\n",
    "    if fs != sampling_rate:\n",
    "        audio = librosa.resample(audio,fs,sampling_rate)\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "# function checking if the size of the inputs are same. If not, then the reference audio's size is adjusted\n",
    "def check_size(audio_ref,audio_test):\n",
    "\n",
    "    if len(audio_ref) > len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "\n",
    "    elif len(audio_ref) < len(audio_test):\n",
    "        print('Durations dont match. Adjusting duration of reference.')\n",
    "        while len(audio_test) > len(audio_ref):\n",
    "            audio_ref = np.append(audio_ref, audio_ref)\n",
    "        audio_ref = audio_ref[:len(audio_test)]\n",
    "\n",
    "    return audio_ref, audio_test\n",
    "\n",
    "\n",
    "# audio loading and feature extraction\n",
    "def feats_loading(test_path, ref_path=None, noresqa_or_noresqaMOS = 0):\n",
    "\n",
    "    if noresqa_or_noresqaMOS == 0 or noresqa_or_noresqaMOS == 1:\n",
    "\n",
    "        audio_ref = audio_loading(ref_path)\n",
    "        audio_test = audio_loading(test_path)\n",
    "        audio_ref, audio_test = check_size(audio_ref,audio_test)\n",
    "\n",
    "        if noresqa_or_noresqaMOS == 0:\n",
    "            ref_feat = extract_stft(audio_ref)\n",
    "            test_feat = extract_stft(audio_test)\n",
    "            return ref_feat,test_feat\n",
    "        else:\n",
    "            return audio_ref, audio_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our directories in which we store recordings or dnn-processed recordings:\n",
    "data_source_dirs=[\n",
    "    '/home/ubuntu/Data/ha_listening_situations/recordings/ku_recordings/',\n",
    "    '/home/ubuntu/Data/ha_listening_situations/processed_m1_alldata_normal/ku_processed/',\n",
    "    '/home/ubuntu/Data/ha_listening_situations/processed_m3_alldata_mild/ku_processed/',\n",
    "    '/home/ubuntu/Data/ha_listening_situations/processed_m4_alldata_normal_causal/ku_processed/',\n",
    "    '/home/ubuntu/Data/ha_listening_situations/processed_m5_alldata_mild_causal/ku_processed/'\n",
    "]\n",
    "\n",
    "# Directory where all recordings used as reference are stored\n",
    "data_ref_dir='/home/ubuntu/Data/ha_listening_situations/recordings/ku_recordings/'\n",
    "\n",
    "# Names of hearing devices\n",
    "ha_models=[\"si3_s\",\"si3_m\",\"si3_p\",\"ph4_s\",\"ph4_m\",\"ph4_p\",\"ph5_s\",\"ph5_m\",\"ph5_p\",\n",
    "           \"gn5_l\",\"gn5_m\",\"gn5_h\",\"gn3_l\",\"gn3_m\",\"gn3_h\"]\n",
    "\n",
    "# Different listening scenes:\n",
    "scenenames=['party_0deg','party_30deg','restaurant_0deg','restaurant_30deg','meeting_0deg','meeting_30deg']\n",
    "\n",
    "# Initialize dataframe, in which each row will represent one pair of plus and minus recording\n",
    "# and all the info and objective measures associated with that pair. \n",
    "recordings_df=pd.DataFrame(columns=['device','dnn_applied','plus_file','minus_file', 'plus_ref_file','minus_ref_file', 'scene'])\n",
    "\n",
    "# initialize lists to be filled inside the loop\n",
    "dnntypes=[]\n",
    "pluses=[]\n",
    "minuses=[]\n",
    "bypass_pluses=[]\n",
    "bypass_minuses=[]\n",
    "rawku_pluses=[]\n",
    "rawku_minuses=[]\n",
    "scenes=[]\n",
    "devices=[]\n",
    "\n",
    "for i, data_source_dir in enumerate(data_source_dirs):\n",
    "    for item in sorted(os.listdir(data_source_dir)):\n",
    "        if os.path.isdir(data_source_dir+item):\n",
    "            recording_name=item\n",
    "            # this line identifies the current device name:\n",
    "            device_id=[model for model in ha_models if model in recording_name]\n",
    "            if len(device_id)>0:\n",
    "                # this line finds the corresponding bypass recording name:\n",
    "                ref_recording_name = [dirname for dirname in os.listdir(data_source_dir)\n",
    "                if os.path.isdir(data_source_dir+dirname) and device_id[0] in dirname and \"bypass\" in dirname][0]\n",
    "            else:\n",
    "                ref_recording_name=recording_name\n",
    "            \n",
    "            for scene in scenenames:\n",
    "                # paths for recorded plus and minus signals (data_source_dir + recording_name)\n",
    "                plusfilepath=data_source_dir+recording_name+'/'+recording_name+'_plus_'+scene+'.wav'\n",
    "                minusfilepath=data_source_dir+recording_name+'/'+recording_name+'_minus_'+scene+'.wav'\n",
    "                # paths for corresponding bypass plus and minus signals (data_ref_dir + ref_recording_name)\n",
    "                bypass_plusrefpath=data_ref_dir+ref_recording_name+'/'+ref_recording_name+'_plus_'+scene+'.wav'\n",
    "                bypass_minusrefpath=data_ref_dir+ref_recording_name+'/'+ref_recording_name+'_minus_'+scene+'.wav'\n",
    "                # paths for corresponding ku100 plus and minus signals \n",
    "                ku_plusrefpath=data_ref_dir+'001_ku100/001_ku100_plus_'+scene+'.wav'\n",
    "                ku_minusrefpath=data_ref_dir+'001_ku100/001_ku100_minus_'+scene+'.wav'\n",
    "                # append all lists:\n",
    "                applieddnntype=i\n",
    "                devices.append(recording_name)\n",
    "                pluses.append(plusfilepath)\n",
    "                minuses.append(minusfilepath)\n",
    "                bypass_pluses.append(bypass_plusrefpath)\n",
    "                bypass_minuses.append(bypass_minusrefpath)\n",
    "                rawku_pluses.append(ku_plusrefpath)\n",
    "                rawku_minuses.append(ku_minusrefpath)\n",
    "                scenes.append(scene)\n",
    "                dnntypes.append(applieddnntype)\n",
    "\n",
    "# fill the data frame with lists appended in the loop above:\n",
    "recordings_df['device']=devices            \n",
    "recordings_df['dnn_applied']=dnntypes\n",
    "recordings_df['plus_file']=pluses\n",
    "recordings_df['minus_file']=minuses\n",
    "recordings_df['plus_ref_bypass']=bypass_pluses\n",
    "recordings_df['minus_ref_bypass']=bypass_minuses\n",
    "recordings_df['plus_ref_ku']=rawku_pluses\n",
    "recordings_df['minus_ref_ku']=rawku_minuses\n",
    "recordings_df['scene']=scenes\n",
    "\n",
    "# Make sure to remove the lines where the \"enabled\" recording is processed and lines where ku100 recording is processed \n",
    "# (we are only interested in dnn-processed bypass recordings):\n",
    "print(f'before: {len(recordings_df)=}')\n",
    "recordings_df=recordings_df[~((recordings_df[\"device\"].str.contains(\"enabled\")) & (recordings_df[\"dnn_applied\"]>0))]\n",
    "recordings_df=recordings_df[~((recordings_df[\"device\"].str.contains(\"001_ku100\")) & (recordings_df[\"dnn_applied\"]>0))]\n",
    "recordings_df=recordings_df[~(recordings_df[\"device\"].str.contains(\"fulldenoising\"))]\n",
    "print(f'after: {len(recordings_df)=}')\n",
    "\n",
    "# Within the group that is not processed by any dnn model we have to distinguish 3 categories and give them labels: \n",
    "# ---> unprocessed reference: ku100 recordings without a hearing aid\n",
    "recordings_df.loc[recordings_df[\"device\"].str.contains(\"001_ku100\"), \"dnn_applied\"]=0.1\n",
    "# ---> unprocessed bypass: recordings with hearing aids in bypass\n",
    "recordings_df.loc[((recordings_df[\"device\"].str.contains(\"bypass\")) & (recordings_df[\"dnn_applied\"]==0)), \"dnn_applied\"]=0.2\n",
    "# ---> unprocessed enabled: recordings with hearing aids enabled\n",
    "recordings_df.loc[((recordings_df[\"device\"].str.contains(\"enabled\")) & (recordings_df[\"dnn_applied\"]==0)), \"dnn_applied\"]=0.3\n",
    "\n",
    "# Check if numbers match: \n",
    "print(f'Number of dnn-processed recordings should be {15*4*6}')\n",
    "print(len(recordings_df.loc[(recordings_df[\"dnn_applied\"]>=1)]))\n",
    "\n",
    "\n",
    "print(f'Number of enabled recordings should be {15*6}')\n",
    "print(len(recordings_df.loc[(recordings_df[\"dnn_applied\"]==0.3)])) \n",
    "\n",
    "\n",
    "print(f'Number of bypass recordings should be {15*6}')\n",
    "print(len(recordings_df.loc[(recordings_df[\"dnn_applied\"]==0.2)])) \n",
    "\n",
    "\n",
    "print(f'Number of ku100 recordings should be {6}')\n",
    "print(len(recordings_df.loc[(recordings_df[\"dnn_applied\"]==0.1)])) \n",
    "\n",
    "# Keep only recordings processed with HA or DNN\n",
    "recordings_only_processed=recordings_df[(recordings_df[\"dnn_applied\"]>0.2)]\n",
    "print(f'Number of all processed recordings should be {15*5*6}')\n",
    "print(len(recordings_only_processed))\n",
    "\n",
    "recordings_only_processed.to_csv('recordings_processed.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_obj_measures(row, ref_method):\n",
    "    # Function: Compute objective measures based on one row of a data frame \n",
    "    # ----- Input: -----\n",
    "    # ref_method - method for computing clean reference signal\n",
    "    # idx - index in the original data frame\n",
    "    # row - row containing filenames of plus and minus recording\n",
    "    # ----- Output: -----\n",
    "    # tuple of objective measures: \n",
    "    # - snr_val: snr estimated with phase inversion technique\n",
    "    # - mbstoi_val: binaural speech intelligibility model\n",
    "    # - hasqi_left_val: hearing aid speech quality for 1 ear\n",
    "    # - hasqi_left_val: hearing aid speech perception index for 1 ear\n",
    "    # - sisdr_val: sudo-rm-rf method for computing sdr\n",
    "    # ----------------------------------------------------------------------\n",
    "    # print sth when the row is done:\n",
    "    FS_TARGET=16e3\n",
    "    fs, plus = wavfile.read(row['plus_file'])\n",
    "    fs, minus = wavfile.read(row['minus_file'])\n",
    "    if fs!=16000:\n",
    "        plus = resample_poly(plus.astype(np.float32), FS_TARGET, fs)\n",
    "        minus = resample_poly(minus.astype(np.float32), FS_TARGET, fs)\n",
    "\n",
    "    \n",
    "    # ------------- Objective measure : SNR -------------\n",
    "    # plus, minus =hlp.synch_sigs(plus,minus)\n",
    "    s=0.5*(plus+minus)\n",
    "    n=0.5*(plus-minus)\n",
    "    snr_L= 10 * np.log10(hlp.power(s[:,0]) / hlp.power(n[:,0]))\n",
    "    snr_R = 10 * np.log10(hlp.power(s[:,1]) / hlp.power(n[:,1]))\n",
    "\n",
    "    # ---------- Compute clean reference signal for further methods ----------\n",
    "\n",
    "    \n",
    "    if ref_method==\"it1\":\n",
    "    # Referencing_method 1 ----> our first iteration\n",
    "        audio_ref=s\n",
    "    \n",
    "    elif ref_method==\"it2\":\n",
    "    # Referencing_method 2 ----> our second iteration \n",
    "        fs, refplus = wavfile.read(row['plus_ref_bypass'])\n",
    "        fs, refminus = wavfile.read(row['minus_ref_bypass'])\n",
    "        if fs!=16000:\n",
    "            refplus = resample_poly(refplus.astype(np.float32), FS_TARGET, fs)\n",
    "            refminus = resample_poly(refminus.astype(np.float32), FS_TARGET, fs)\n",
    "        # refplus, refminus =hlp.synch_sigs(refplus,refminus)\n",
    "        audio_ref=0.5*(refplus+refminus)\n",
    "\n",
    "    elif ref_method==\"it3\":\n",
    "    # Referencing_method 3 ----> our third iteration \n",
    "        fs, refplus = wavfile.read(row['plus_ref_ku'])\n",
    "        fs, refminus = wavfile.read(row['minus_ref_ku'])\n",
    "        if fs!=16000:\n",
    "            refplus = resample_poly(refplus.astype(np.float32), FS_TARGET, fs)\n",
    "            refminus = resample_poly(refminus.astype(np.float32), FS_TARGET, fs)\n",
    "        # refplus, refminus =hlp.synch_sigs(refplus,refminus)\n",
    "        audio_ref=0.5*(refplus+refminus)\n",
    "\n",
    "    audio_ref = audio_ref[:,0]\n",
    "    audio_test = plus[:,0]\n",
    "    audio_ref, audio_test = check_size(audio_ref,audio_test)\n",
    "\n",
    "    \n",
    "    if metric_type == 0:\n",
    "        nmr_feat = extract_stft(audio_ref)\n",
    "        test_feat = extract_stft(audio_test)\n",
    "    else:\n",
    "        nmr_feat = audio_ref\n",
    "        test_feat = audio_test\n",
    "        \n",
    "    test_feat = torch.from_numpy(test_feat).float().to(device).unsqueeze(0)\n",
    "    nmr_feat = torch.from_numpy(nmr_feat).float().to(device).unsqueeze(0)\n",
    "    if metric_type == 0:\n",
    "        noresqa_pout, noresqa_qout = model_prediction_noresqa(test_feat, nmr_feat)\n",
    "        #print('Probaility of the test speech cleaner than the given NMR =', noresqa_pout)\n",
    "        #print('NORESQA score of the test speech with respect to the given NMR =', noresqa_qout)\n",
    "        #return noresqa_pout, noresqa_quout\n",
    "    elif metric_type == 1:\n",
    "        mos_score = model_prediction_noresqa_mos(test_feat, nmr_feat)\n",
    "        #print('MOS score of the test speech (assuming NMR is clean) =', str(5.0-mos_score))    #return idx, snr_L, snr_R, hasqi_L, hasqi_R, haspi_L, haspi_R, sisdr_L, sisdr_R, snrloss_L, snrloss_R, mbstoi_B \n",
    "        #return mos_score\n",
    "    \n",
    "    return mos_score\n",
    "# Test function compute_obj_measures():\n",
    "measures=pd.read_csv('recordings_processed.csv')\n",
    "\n",
    "#compute_obj_measures(120,measures.loc[120],\"it1\")\n",
    "#print(f'Measures for {measures.loc[idx,\"plus_file\"]}\\n{snr_L=}\\n{hasqi_L=}\\n{haspi_L=}\\n{sisdr_L=}\\n{snrloss_L=}\\n{mbstoi_B=}')\n",
    "\n",
    "# Multiprocessing - compute measures\n",
    "# The main script has to be in the same cell as the definition of the function\n",
    "# if __name__ == '__main__':\n",
    "#     NUM_OF_WORKERS = 8\n",
    "#     with Pool(NUM_OF_WORKERS) as pool:\n",
    "#         results = [pool.apply_async(compute_obj_measures, [idx, row, \"it3\"]) for idx, row in measures.iterrows()]\n",
    "#         for result in results:\n",
    "#             idx, snr_L, snr_R, hasqi_L, hasqi_R, haspi_L, haspi_R, sisdr_L, sisdr_R, snrloss_L, snrloss_R, mbstoi_B  = result.get()\n",
    "#             measures.loc[idx, 'snr_L'] = snr_L\n",
    "#             measures.loc[idx, 'snr_R'] = snr_R\n",
    "#             measures.loc[idx, 'haspi_L'] = haspi_L\n",
    "#             measures.loc[idx, 'haspi_R'] = haspi_R\n",
    "#             measures.loc[idx, 'hasqi_L'] = hasqi_L\n",
    "#             measures.loc[idx, 'hasqi_R'] = hasqi_R\n",
    "#             measures.loc[idx, 'sisdr_L'] = sisdr_L\n",
    "#             measures.loc[idx, 'sisdr_R'] = sisdr_R\n",
    "#             measures.loc[idx, 'snrloss_L'] = snrloss_L\n",
    "#             measures.loc[idx, 'snrloss_R'] = snrloss_R\n",
    "#             measures.loc[idx, 'mbstoi_B'] = mbstoi_B\n",
    "\n",
    "# measures.to_csv('objective_measures3_nosynch.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT1\n",
    "mos_it1 = []\n",
    "for i in tqdm.tqdm(range(len(measures))):\n",
    "    row = measures.loc[i]\n",
    "    mos = compute_obj_measures(row, 'it1')\n",
    "    mos_it1.append(mos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT2\n",
    "mos_it2 = []\n",
    "for i in tqdm.tqdm(range(len(measures))):\n",
    "    row = measures.loc[i]\n",
    "    mos = compute_obj_measures(row, 'it2')\n",
    "    mos_it2.append(mos)\n",
    "    \n",
    "# IT3\n",
    "mos_it3 = []\n",
    "for i in tqdm.tqdm(range(len(measures))):\n",
    "    row = measures.loc[i]\n",
    "    mos = compute_obj_measures(row, 'it3')\n",
    "    mos_it3.append(mos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_computed=pd.read_csv('objective_measures3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_computed['noresqaMOS']=mos_it3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_computed=pd.read_csv('objective_measures2.csv')\n",
    "measures_computed['noresqaMOS']=mos_it2\n",
    "measures_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synch_sigs(sig1,sig2):\n",
    "    sig1_out=np.zeros(sig1.shape)\n",
    "    sig2_out=np.zeros(sig2.shape)\n",
    "    corr = signal.correlate(sig1[:,0], sig2[:,0], 'full')\n",
    "    lag = signal.correlation_lags(len(sig1[:,0]), len(sig2[:,0]), mode='full')[np.argmax(corr)]\n",
    "    if lag > 0:\n",
    "        sig2=sig2[0:-lag, :]\n",
    "        sig1=sig1[lag:, :]\n",
    "    elif lag < 0:\n",
    "        sig2=sig2[-lag:, :]\n",
    "        sig1=sig1[0:lag, :]\n",
    "\n",
    "    sig1_out[:sig1.shape[0],:]=sig1\n",
    "    sig2_out[:sig2.shape[0],:]=sig2\n",
    "    return sig1_out,sig2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_computed=pd.read_csv('objective_measures1.csv')\n",
    "measures_computed['noresqaMOS']=mos_it1\n",
    "measures_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "measures_computed=pd.read_csv('objective_measures3.csv')\n",
    "\n",
    "# For a better visibility, we add one column which specifies the hearing aid model (without division into different receivers)\n",
    "ha_models=[\"gn3\",\"gn5\",\"ph5\",\"ph4\",\"si3\"]\n",
    "for modelname in ha_models:\n",
    "    measures_computed.loc[measures_computed['device'].str.contains(modelname), \"device_group\"]=modelname\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "  \n",
    "# Settings the warnings to be ignored\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_1_measure(df,measure):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(25, 5), sharey=True) \n",
    "    sns.set(font_scale=0.8)\n",
    "    sns.stripplot(ax=axes[1],data=df,x=\"dnn_applied\", y=measure, hue=\"device_group\")\n",
    "    sns.stripplot(ax=axes[2],data=df,x=\"dnn_applied\", y=measure, hue=\"scene\")\n",
    "    df.boxplot(ax=axes[0],column=measure,by=\"dnn_applied\")\n",
    "    axes[0].set_xlabel('')\n",
    "    axes[1].set_xlabel('')\n",
    "    axes[2].set_xlabel('')\n",
    "    axes[0].set_title('Standard boxplot')\n",
    "    axes[1].set_title('Colored by devices')\n",
    "    axes[2].set_title('Colored by scene')\n",
    "    axes[0].set_xticklabels(['HA','DNN-normal','DNN-mild','DNN-normal_C','DNN-mild_C'], rotation=45)\n",
    "    axes[1].set_xticklabels(['HA','DNN-normal','DNN-mild','DNN-normal_C','DNN-mild_C'], rotation=45)\n",
    "    axes[2].set_xticklabels(['HA','DNN-normal','DNN-mild','DNN-normal_C','DNN-mild_C'], rotation=45)\n",
    "    fig.suptitle('Measure: '+ measure)\n",
    "    plt.show()\n",
    "\n",
    "# Plot all measures:\n",
    "plot_1_measure(measures_computed,\"snr_L\")\n",
    "plot_1_measure(measures_computed,\"sisdr_L\")\n",
    "plot_1_measure(measures_computed,\"snrloss_L\")\n",
    "plot_1_measure(measures_computed,\"hasqi_L\")\n",
    "plot_1_measure(measures_computed,\"haspi_L\")\n",
    "plot_1_measure(measures_computed,\"mbstoi_B\")\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
